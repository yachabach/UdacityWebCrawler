Written Questions

Q1. Run the web crawler using the configurations located at src/main/config/written_question_1a.json and
    src/main/config/written_question_1b.json. The only difference between these configurations is that one always uses
    the sequential crawler and the other always uses the parallel crawler. Inspect the profile output in
    profileData.txt.

    If you are using a multi-processor computer, you should notice that SequentialWebCrawler#crawl and
    ParallelWebCrawler#crawl took about the same amount of time, but PageParserImpl#parse took much longer when run with
    the ParallelWebCrawler.

    Why did the parser take more time when run with ParallelWebCrawler?

    The parallel crawler was slower than the sequential crawler by 25ms.  This is possibly because
    the parallel crawler requires additional overhead to manage thread pools.

    Run at Thu, 7 Oct 2021 22:32:19 GMT
    com.udacity.webcrawler.SequentialWebCrawler#crawl took 0m 2s 299ms
    com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 299ms

    Run at Thu, 7 Oct 2021 22:32:26 GMT
    com.udacity.webcrawler.ParallelWebCrawler#crawl took 0m 2s 339ms
    com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 324ms
    ---------------------------------------


Q2. Your manager ran your crawler on her old personal computer, using the configurations from Q1, and she notices that
    the sequential crawler actually outperforms the parallel crawler. She would like to know why.

    (a) Suggest one reason why the sequential web crawler was able to read more web pages than the parallel crawler.
        (Hint: Try setting "parallelism" to 1 in the JSON configs to simulate your manager's computer.)

    If my manager reached more pages with the sequential crawler it most likely was because the overhead associated
    with multithreading took time from the parallel implementation.  I set the parallelism to 1 and ran the following
    sequences:

    Fourth run:
    - 1a
    - 1b
    Run at Thu, 7 Oct 2021 22:38:52 GMT
    com.udacity.webcrawler.SequentialWebCrawler#crawl took 0m 2s 352ms
    com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 321ms

    Run at Thu, 7 Oct 2021 22:39:00 GMT
    com.udacity.webcrawler.ParallelWebCrawler#crawl took 0m 2s 164ms
    com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 146ms
    ------------------------------------

    Fifth run:
    - 1b
    - 1a
    Run at Thu, 7 Oct 2021 22:39:29 GMT
    com.udacity.webcrawler.ParallelWebCrawler#crawl took 0m 2s 563ms
    com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 546ms

    Run at Thu, 7 Oct 2021 22:39:37 GMT
    com.udacity.webcrawler.SequentialWebCrawler#crawl took 0m 2s 402ms
    com.udacity.webcrawler.parser.PageParserImpl#parse took 0m 2s 386ms
    ---------------------------

    In both cases, the second implementation was faster than the first.  Also, in this scenario, both implementations were
    able to reach 5 urls in the allotted 2 seconds.  This is probably because of caching on my modern computer.

    (b) Suggest one scenario in which the parallel web crawler will almost certainly perform better than the sequential
        crawler. Why will it perform better?

    I tested the crawlers with longer timeouts.  The parallel crawler was able to access 6 additional sites in the
    single thread case and 8 additional sites with multithreading.  This is probably because additional sites could be
    accessed while waiting for other sites to respond.  In the case where crawled sites are slow to respond, the
    parallel crawler will always work faster.


Q3. Analyze your method profiler through the lens of Aspect Oriented Programming, by answering the following questions:

    (a) What cross-cutting concern is being addressed by the com.udacity.webcrawler.profiler.Profiler class?

    Performance profiling is the cross-cutting concern addressed by the profiler.

    (b) What are the join points of the Profiler in the web crawler program?

    The join points are all the methods annotated with the @Profiled annotation.  These include:
     - crawl method of the WebCrawler
     - parse method of the PageParser


Q4. Identify three (3) different design patterns used in this project, and explain which interfaces, classes, and/or
    libraries use or implement those design patterns.

    Three design patterns in this project include Creation patterns, Strategy pattern, and Dependency Injection:

    Builder Pattern
    - Classes - CrawlResult, PageParser.Result, CrawlerConfiguration and CrawlAction
    - Likes - They are easy to understand and relieve you of the need to remember the order of construction params
    - Dislikes - They add many lines of code and complexity

    Strategy Pattern
    - Classes - WebCrawler, SequentialWebCrawler, ParallelWebCrawler
    - Likes - allows us to try different implementations without changing a lot of code
    - Dislikes - VERY complicated and difficult to keep all the layers straight in my head

    Dependency Injection
    - Classes - PageParser, Profiler, numerous variables
    - Likes - allows us to use objects without having to construct them; cleans the code in a class
    - Dislikes - VERY hard to understand Guice's syntax.  Acts like global variables, it's hard to know their source

    For each pattern, name one thing about the pattern that you LIKED, and one thing you DISLIKED. If you did not like
    anything, you can name two things you disliked.



